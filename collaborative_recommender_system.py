# -*- coding: utf-8 -*-
"""collaborative_recommender_system.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lM9i7rZQiHNDjhDQVg_uepeAiZ05E0Nr

## collaborative_recommender_system

### KNN
"""

!rm -R /content/music-recommendation-system
!git clone https://github.com/sheemachinnu/music-recommendation-system

import os

files = sorted(os.listdir('./music-recommendation-system/datasets/million_dataset'))
n = len(files)

with open('million_dataset.txt', 'w') as nf:
    for i in range(n):
        f = open("./music-recommendation-system/datasets/million_dataset/"+files[i])

        nf.write(''.join(f.readlines()))

        f.close()

import os

files = sorted(os.listdir('./music-recommendation-system/datasets/song_data'))
n = len(files)

with open('song_data.csv', 'w') as nf:
    for i in range(n):
        f = open("./music-recommendation-system/datasets/song_data/"+files[i])

        nf.write(''.join(f.readlines()))

        f.close()



!pip install fuzzywuzzy

# KNN Recommender
from sklearn.neighbors import NearestNeighbors
from fuzzywuzzy import fuzz
import numpy as np

class Recommender:
    def __init__(self, metric, algorithm, k, data, decode_id_song):
        self.metric = metric
        self.algorithm = algorithm
        self.k = k
        self.data = data
        self.decode_id_song = decode_id_song
        self.data = data
        self.model = self._recommender().fit(data)
    
    def make_recommendation(self, new_song, n_recommendations):
        recommended = self._recommend(new_song=new_song, n_recommendations=n_recommendations) 
        print("... Done")
        return recommended 
    
    def _recommender(self):
        return NearestNeighbors(metric=self.metric, algorithm=self.algorithm, n_neighbors=self.k, n_jobs=-1)
    
    def _recommend(self, new_song, n_recommendations):
        # Get the id of the recommended songs
        recommendations = []
        recommendation_ids = self._get_recommendations(new_song=new_song, n_recommendations=n_recommendations)
        # return the name of the song using a mapping dictionary
        recommendations_map = self._map_indeces_to_song_title(recommendation_ids)
        # Translate this recommendations into the ranking of song titles recommended
        for i, (idx, dist) in enumerate(recommendation_ids):
            recommendations.append(recommendations_map[idx])
        return recommendations
                 
    def _get_recommendations(self, new_song, n_recommendations):
        # Get the id of the song according to the text
        recom_song_id = self._fuzzy_matching(song=new_song)
        # Start the recommendation process
        print(f"Starting the recommendation process for {new_song} ...")
        # Return the n neighbors for the song id
        distances, indices = self.model.kneighbors(self.data[recom_song_id], n_neighbors=n_recommendations+1)
        return sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())), key=lambda x: x[1])[:0:-1]
    
    def _map_indeces_to_song_title(self, recommendation_ids):
        # get reverse mapper
        return {song_id: song_title for song_title, song_id in self.decode_id_song.items()}
    
    def _fuzzy_matching(self, song):
        match_tuple = []
        # get match
        for title, idx in self.decode_id_song.items():
            ratio = fuzz.ratio(title.lower(), song.lower())
            if ratio >= 60:
                match_tuple.append((title, idx, ratio))
        # sort
        match_tuple = sorted(match_tuple, key=lambda x: x[2])[::-1]
        if not match_tuple:
            print(f"The recommendation system could not find a match for {song}")
            return
        return match_tuple[0][1]

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.sparse import csr_matrix

#Read userid-songid-listen_count
song_info = pd.read_csv('./million_dataset.txt',sep='\t',header=None)
# print (song_info)
song_info.columns = ['user_id', 'song_id', 'listen_count']

#Read song  metadata
song_actual =  pd.read_csv('./song_data.csv')
song_actual.drop_duplicates(['song_id'], inplace=True)

#Merge the two dataframes above to create input dataframe for recommender systems
songs = pd.merge(left=song_info, right=song_actual, on="song_id", how="left")

songs.head()

songs.to_csv('songs.csv', index=False)

df_songs = pd.read_csv('songs.csv')

df_songs.head()

#Get total observations
print(f"There are {df_songs.shape[0]} observations in the dataset")

df_songs.isnull().sum()

df_songs.dtypes

#Unique songs
unique_songs = df_songs['title'].unique().shape[0]
print(f"There are {unique_songs} unique songs in the dataset")

#Unique artists
unique_artists = df_songs['artist_name'].unique().shape[0]
print(f"There are {unique_artists} unique artists in the dataset")

#Unique users
unique_users = df_songs['user_id'].unique().shape[0]
print(f"There are {unique_users} unique users in the dataset")

"""#### Most popular songs"""

#count how many rows we have by song, we show only the ten more popular songs 
ten_pop_songs = df_songs.groupby('title')['listen_count'].count().reset_index().sort_values(['listen_count', 'title'], ascending = [0,1])
ten_pop_songs['percentage']  = round(ten_pop_songs['listen_count'].div(ten_pop_songs['listen_count'].sum())*100, 2)

ten_pop_songs = ten_pop_songs[:10]
ten_pop_songs

labels = ten_pop_songs['title'].tolist()
counts = ten_pop_songs['listen_count'].tolist()

plt.figure()
sns.barplot(x=counts, y=labels, palette='Set3')
sns.despine(left=True, bottom=True)

"""#### Most popular artist"""

#count how many rows we have by artist name, we show only the ten more popular artist 
ten_pop_artists  = df_songs.groupby(['artist_name'])['listen_count'].count().reset_index().sort_values(['listen_count', 'artist_name'], 
                                                                                                ascending = [0,1])

ten_pop_artists = ten_pop_artists[:10]
ten_pop_artists

plt.figure()
labels = ten_pop_artists['artist_name'].tolist()
counts = ten_pop_artists['listen_count'].tolist()
sns.barplot(x=counts, y=labels, palette='Set2')
sns.despine(left=True, bottom=True)

"""#### Listen count by user"""

listen_counts = pd.DataFrame(df_songs.groupby('listen_count').size(), columns=['count'])
listen_counts.head()

print(f"The maximum time the same user listened to the same songs was: {listen_counts.reset_index(drop=False)['listen_count'].iloc[-1]}")

print(f"On average, a user listen to the same song {df_songs['listen_count'].mean()} times")

plt.figure(figsize=(20, 5))
sns.boxplot(x='listen_count', data=df_songs)
sns.despine()

# What are the most frequent number of times a user listen to the same song?
listen_counts_temp = listen_counts[listen_counts['count'] > 50].reset_index(drop=False)

plt.figure(figsize=(16, 8))
sns.barplot(x='listen_count', y='count', palette='Set3', data=listen_counts_temp)
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.show();

# How many songs does a user listen in average?
song_user = df_songs.groupby('user_id')['song_id'].count()
# song_user
plt.figure(figsize=(16, 8))
sns.distplot(song_user.values, color='orange')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.show();

print(f"A user listens to an average of {np.mean(song_user)} songs")

print(f"A user listens to an average of {np.median(song_user)} songs, with minimum {np.min(song_user)} and maximum {np.max(song_user)} songs")

# Get how many values should it be if all songs have been listen by all users
values_matrix = unique_users * unique_songs

# Substract the total values with the actural shape of the DataFrame songs
zero_values_matrix = values_matrix - df_songs.shape[0]

print(f"The matrix of users x songs has {zero_values_matrix} values that are zero")

"""#### Prepare the data"""

# Get users which have listen to at least 16 songs
song_ten_id = song_user[song_user > 16].index.to_list()

# Filtered the dataset to keep only those users with more than 16 listened
df_song_id_more_ten = df_songs[df_songs['user_id'].isin(song_ten_id)].reset_index(drop=True)

# convert the dataframe into a pivot table
df_songs_features = df_song_id_more_ten.pivot(index='song_id', columns='user_id', values='listen_count').fillna(0)
#df_songs_features.isnull().sum()
df_songs_features.head()

# obtain a sparse matrix
mat_songs_features = csr_matrix(df_songs_features.values)

test = pd.DataFrame(mat_songs_features)
test.head()
#mat_songs_features

df_unique_songs = df_songs.drop_duplicates(subset=['song_id']).reset_index(drop=True)[['song_id', 'title']]

decode_id_song = {
    song: i for i, song in 
    enumerate(list(df_unique_songs.set_index('song_id').loc[df_songs_features.index].title))
}
#df_unique_songs.head()
print(decode_id_song)

"""#### Model and recommendations"""

model = Recommender(metric='cosine', algorithm='brute', k=20, data=mat_songs_features, decode_id_song=decode_id_song)

song = 'Dog Days Are Over (Radio Edit)'
# song = 'Ignorance (Album Version)'

new_recommendations = model.make_recommendation(new_song=song, n_recommendations=10)

print(f"The recommendations for {song} are:")
print(f"{new_recommendations}")



songs['title']









